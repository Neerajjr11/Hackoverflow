{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "with open(r'C:\\NEERAJ\\HACKATHON\\HACKOVERFLOW\\Resume-Scoring-using-NLP\\skills.txt',encoding=\"utf8\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science is a \"concept to unify statistics, data analysis, machine learning and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science. Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for line in content:\n",
    "    tokens=word_tokenize(line)\n",
    "    tok=[w.lower() for w in tokens]\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    strpp=[w.translate(table) for w in tok]\n",
    "    words=[word for word in strpp if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[w for w in words if not w in stop_words]\n",
    "    x.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['william', 'cleveland', 'introduced', 'data', 'science', 'independent', 'discipline', 'extending', 'field', 'statistics', 'incorporate', 'advances', 'computing', 'data', 'article', 'data', 'science', 'action', 'plan', 'expanding', 'technical', 'areas', 'field', 'statistics', 'published', 'volume', 'april', 'edition', 'international', 'statistical', 'review', 'revue', 'internationale', 'de', 'statistique', 'report', 'cleveland', 'establishes', 'six', 'technical', 'areas', 'believed', 'encompass', 'field', 'data', 'science', 'multidisciplinary', 'investigations', 'models', 'methods', 'data', 'computing', 'data', 'pedagogy', 'tool', 'evaluation', 'theory']\n"
     ]
    }
   ],
   "source": [
    "print(texts[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing commonly occuring words that are not useful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['william', 'cleveland', 'introduced', 'data', 'science', 'independent', 'discipline', 'extending', 'field', 'statistics', 'incorporate', 'advances', 'computing', 'data', 'article', 'data', 'science', 'action', 'plan', 'expanding', 'technical', 'areas', 'field', 'statistics', 'published', 'volume', 'april', 'edition', 'international', 'statistical', 'review', 'revue', 'internationale', 'de', 'statistique', 'report', 'cleveland', 'establishes', 'six', 'technical', 'areas', 'believed', 'encompass', 'field', 'data', 'science', 'multidisciplinary', 'investigations', 'models', 'methods', 'data', 'computing', 'data', 'pedagogy', 'tool', 'evaluation', 'theory']\n"
     ]
    }
   ],
   "source": [
    "with open('common.txt') as f:\n",
    "    content2 = f.read()\n",
    "ntexts=[]\n",
    "l=len(texts)\n",
    "for j in range(l):\n",
    "    s=texts[j]\n",
    "    res = [i for i in s if i not in content2]\n",
    "    ntexts.append(res)\n",
    "print(texts[6])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['william', 'cleveland', 'introduced', 'data', 'science', 'independent', 'discipline', 'extending', 'field', 'statistics', 'incorporate', 'advances', 'computing', 'data', 'article', 'data', 'science', 'action', 'plan', 'expanding', 'technical', 'areas', 'field', 'statistics', 'published', 'volume', 'april', 'edition', 'international', 'statistical', 'review', 'revue', 'internationale', 'de', 'statistique', 'report', 'cleveland', 'establishes', 'six', 'technical', 'areas', 'believed', 'encompass', 'field', 'data', 'science', 'multidisciplinary', 'investigations', 'models', 'methods', 'data', 'computing', 'data', 'pedagogy', 'tool', 'evaluation', 'theory']\n"
     ]
    }
   ],
   "source": [
    "print(ntexts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    }
   ],
   "source": [
    "print(len(ntexts))\n",
    "texts=ntexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286\n"
     ]
    }
   ],
   "source": [
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "x=ntexts\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "phrases = Phrases(x, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences\n",
    "all_sentences = list(bigram[x])\n",
    "model=gensim.models.Word2Vec(all_sentences,size=5000,min_count=2,workers=4,window=4)\n",
    "model.save(\"final.model\")\n",
    "wrds=list(model.wv.vocab)\n",
    "print(len(wrds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\NEERAJ\\HACKATHON\\HACKOVERFLOW\\Resume-Scoring-using-NLP\\NLP_Review Final.ipynb Cell 22\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/NEERAJ/HACKATHON/HACKOVERFLOW/Resume-Scoring-using-NLP/NLP_Review%20Final.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m z\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mmost_similar(\u001b[39m\"\u001b[39m\u001b[39mmachine_learning\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "z=model.wv.most_similar(\"machine_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 0.1716945916414261), ('used', 0.16244381666183472), ('software', 0.16178086400032043), ('deep_learning', 0.16104692220687866), ('programming', 0.15752632915973663), ('c', 0.15238842368125916), ('deep', 0.14894665777683258), ('neural_networks', 0.14388877153396606), ('layer', 0.14237236976623535), ('processing', 0.1394120156764984)]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import collections\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Resumes from folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath=r'C:\\NEERAJ\\HACKATHON\\HACKOVERFLOW\\Resume-Scoring-using-NLP\\Resumes'\n",
    "#Path for the files\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to words from resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def pdfextract(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = collections.Counter(range(number_of_pages))\n",
    "    for i in c:\n",
    "        #page\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content = page.extractText()\n",
    "    return (page_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words=[k[0] for k in model.wv.most_similar(\"machine_learning\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram(words):\n",
    "    common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "    x=words.split()\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "    phrases = Phrases(x, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "    bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "    all_sentences = list(bigram[x])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to build candidate profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profile(file):\n",
    "    model=Word2Vec.load(\"final.model\")\n",
    "    text = str(pdfextract(file))\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    #text=create_bigram(text)\n",
    "    #print(text)\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    #keyword_dictionary = pd.read_csv(r'C:\\Users\\dell\\Desktop\\New folder\\ML_CS\\NLP\\technical_skills.csv')\n",
    "    stats = [nlp(text[0]) for text in model.wv.most_similar(\"statistics\")]\n",
    "    NLP = [nlp(text[0]) for text in model.wv.most_similar(\"language\")]\n",
    "    ML = [nlp(text[0]) for text in model.wv.most_similar(\"machine_learning\")]\n",
    "    DL = [nlp(text[0]) for text in model.wv.most_similar(\"deep\")]\n",
    "    #R = [nlp(text) for text in keyword_dictionary['R Language'].dropna(axis = 0)]\n",
    "    python = [nlp(text[0]) for text in model.wv.most_similar(\"python\")]\n",
    "    Data_Engineering = [nlp(text[0]) for text in model.wv.most_similar(\"data\")]\n",
    "    print(\"*******************************************\")\n",
    "    #print(stats_words,NLP_words)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats)\n",
    "    matcher.add('NLP', None, *NLP)\n",
    "    matcher.add('ML', None, *ML)\n",
    "    matcher.add('DL', None, *DL)\n",
    "    matcher.add('Python', None, *python)\n",
    "    matcher.add('DE', None, *Data_Engineering)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode I\n",
    "        span = doc[start : end]               # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    print(\"KEYWORDS\")\n",
    "    print(keywords)\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    print(\"********************DF********************\")\n",
    "    print(df)\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "    \n",
    "       \n",
    "    name = filename.split('_')\n",
    "    print(name)\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "    print(\"******************DATAF**************\")\n",
    "    print(dataf)\n",
    "\n",
    "    return(dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code to execute the above functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "KEYWORDS\n",
      "ML c (86)\n",
      "NLP c (86)\n",
      "DE c (86)\n",
      "DL c (86)\n",
      "Python c (86)\n",
      "********************DF********************\n",
      "   Keywords_List\n",
      "0      ML c (86)\n",
      "1     NLP c (86)\n",
      "2      DE c (86)\n",
      "3      DL c (86)\n",
      "4  Python c (86)\n",
      "['AbhayArora']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject Keyword Count\n",
      "0     abhayarora      ML      c     86\n",
      "1     abhayarora     NLP      c     86\n",
      "2     abhayarora      DE      c     86\n",
      "3     abhayarora      DL      c     86\n",
      "4     abhayarora  Python      c     86\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "NLP may (3)\n",
      "Stats may (3)\n",
      "DL may (3)\n",
      "Python may (3)\n",
      "ML programming (1)\n",
      "DL programming (1)\n",
      "Python programming (1)\n",
      "NLP systems (1)\n",
      "DL systems (1)\n",
      "ML software (1)\n",
      "NLP software (1)\n",
      "Stats software (1)\n",
      "DE software (1)\n",
      "DL software (1)\n",
      "Python software (1)\n",
      "ML data (3)\n",
      "NLP data (3)\n",
      "Stats data (3)\n",
      "DL data (3)\n",
      "Python data (3)\n",
      "ML c (1)\n",
      "NLP c (1)\n",
      "DE c (1)\n",
      "DL c (1)\n",
      "Python c (1)\n",
      "********************DF********************\n",
      "             Keywords_List\n",
      "0              NLP may (3)\n",
      "1            Stats may (3)\n",
      "2               DL may (3)\n",
      "3           Python may (3)\n",
      "4       ML programming (1)\n",
      "5       DL programming (1)\n",
      "6   Python programming (1)\n",
      "7          NLP systems (1)\n",
      "8           DL systems (1)\n",
      "9          ML software (1)\n",
      "10        NLP software (1)\n",
      "11      Stats software (1)\n",
      "12         DE software (1)\n",
      "13         DL software (1)\n",
      "14     Python software (1)\n",
      "15             ML data (3)\n",
      "16            NLP data (3)\n",
      "17          Stats data (3)\n",
      "18             DL data (3)\n",
      "19         Python data (3)\n",
      "20                ML c (1)\n",
      "21               NLP c (1)\n",
      "22                DE c (1)\n",
      "23                DL c (1)\n",
      "24            Python c (1)\n",
      "['AmanSharma']\n",
      "******************DATAF**************\n",
      "   Candidate Name Subject       Keyword Count\n",
      "0      amansharma     NLP          may      3\n",
      "1      amansharma   Stats          may      3\n",
      "2      amansharma      DL          may      3\n",
      "3      amansharma  Python          may      3\n",
      "4      amansharma      ML  programming      1\n",
      "5      amansharma      DL  programming      1\n",
      "6      amansharma  Python  programming      1\n",
      "7      amansharma     NLP      systems      1\n",
      "8      amansharma      DL      systems      1\n",
      "9      amansharma      ML     software      1\n",
      "10     amansharma     NLP     software      1\n",
      "11     amansharma   Stats     software      1\n",
      "12     amansharma      DE     software      1\n",
      "13     amansharma      DL     software      1\n",
      "14     amansharma  Python     software      1\n",
      "15     amansharma      ML         data      3\n",
      "16     amansharma     NLP         data      3\n",
      "17     amansharma   Stats         data      3\n",
      "18     amansharma      DL         data      3\n",
      "19     amansharma  Python         data      3\n",
      "20     amansharma      ML            c      1\n",
      "21     amansharma     NLP            c      1\n",
      "22     amansharma      DE            c      1\n",
      "23     amansharma      DL            c      1\n",
      "24     amansharma  Python            c      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "NLP using (4)\n",
      "Stats image (1)\n",
      "ML processing (1)\n",
      "********************DF********************\n",
      "       Keywords_List\n",
      "0      NLP using (4)\n",
      "1    Stats image (1)\n",
      "2  ML processing (1)\n",
      "['Chandler']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject      Keyword Count\n",
      "0       chandler     NLP       using      4\n",
      "1       chandler   Stats       image      1\n",
      "2       chandler      ML  processing      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "ML data (1)\n",
      "NLP data (1)\n",
      "Stats data (1)\n",
      "DL data (1)\n",
      "Python data (1)\n",
      "NLP using (3)\n",
      "Stats image (1)\n",
      "********************DF********************\n",
      "     Keywords_List\n",
      "0      ML data (1)\n",
      "1     NLP data (1)\n",
      "2   Stats data (1)\n",
      "3      DL data (1)\n",
      "4  Python data (1)\n",
      "5    NLP using (3)\n",
      "6  Stats image (1)\n",
      "['MeghnaLohani']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject Keyword Count\n",
      "0   meghnalohani      ML   data      1\n",
      "1   meghnalohani     NLP   data      1\n",
      "2   meghnalohani   Stats   data      1\n",
      "3   meghnalohani      DL   data      1\n",
      "4   meghnalohani  Python   data      1\n",
      "5   meghnalohani     NLP  using      3\n",
      "6   meghnalohani   Stats  image      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "ML used (2)\n",
      "DE used (2)\n",
      "Stats system (3)\n",
      "NLP using (12)\n",
      "DE learning (2)\n",
      "DL models (2)\n",
      "Stats image (3)\n",
      "Stats new (1)\n",
      "NLP new (1)\n",
      "DE new (1)\n",
      "ML data (1)\n",
      "NLP data (1)\n",
      "Stats data (1)\n",
      "DL data (1)\n",
      "Python data (1)\n",
      "********************DF********************\n",
      "       Keywords_List\n",
      "0        ML used (2)\n",
      "1        DE used (2)\n",
      "2   Stats system (3)\n",
      "3     NLP using (12)\n",
      "4    DE learning (2)\n",
      "5      DL models (2)\n",
      "6    Stats image (3)\n",
      "7      Stats new (1)\n",
      "8        NLP new (1)\n",
      "9         DE new (1)\n",
      "10       ML data (1)\n",
      "11      NLP data (1)\n",
      "12    Stats data (1)\n",
      "13       DL data (1)\n",
      "14   Python data (1)\n",
      "['Neeraj']\n",
      "******************DATAF**************\n",
      "   Candidate Name Subject    Keyword Count\n",
      "0          neeraj      ML      used      2\n",
      "1          neeraj      DE      used      2\n",
      "2          neeraj   Stats    system      3\n",
      "3          neeraj     NLP     using     12\n",
      "4          neeraj      DE  learning      2\n",
      "5          neeraj      DL    models      2\n",
      "6          neeraj   Stats     image      3\n",
      "7          neeraj   Stats       new      1\n",
      "8          neeraj     NLP       new      1\n",
      "9          neeraj      DE       new      1\n",
      "10         neeraj      ML      data      1\n",
      "11         neeraj     NLP      data      1\n",
      "12         neeraj   Stats      data      1\n",
      "13         neeraj      DL      data      1\n",
      "14         neeraj  Python      data      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "NLP using (5)\n",
      "Stats image (1)\n",
      "ML processing (1)\n",
      "ML deep (1)\n",
      "Python deep (1)\n",
      "DE learning (1)\n",
      "********************DF********************\n",
      "       Keywords_List\n",
      "0      NLP using (5)\n",
      "1    Stats image (1)\n",
      "2  ML processing (1)\n",
      "3        ML deep (1)\n",
      "4    Python deep (1)\n",
      "5    DE learning (1)\n",
      "['Phoebe Buffay']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject      Keyword Count\n",
      "0  phoebe buffay     NLP       using      5\n",
      "1  phoebe buffay   Stats       image      1\n",
      "2  phoebe buffay      ML  processing      1\n",
      "3  phoebe buffay      ML        deep      1\n",
      "4  phoebe buffay  Python        deep      1\n",
      "5  phoebe buffay      DE    learning      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "ML used (3)\n",
      "DE used (3)\n",
      "ML c (2)\n",
      "NLP c (2)\n",
      "DE c (2)\n",
      "DL c (2)\n",
      "Python c (2)\n",
      "ML programming (1)\n",
      "DL programming (1)\n",
      "Python programming (1)\n",
      "NLP may (1)\n",
      "Stats may (1)\n",
      "DL may (1)\n",
      "Python may (1)\n",
      "DE learning (3)\n",
      "ML deep (1)\n",
      "Python deep (1)\n",
      "ML data (3)\n",
      "NLP data (3)\n",
      "Stats data (3)\n",
      "DL data (3)\n",
      "Python data (3)\n",
      "NLP using (1)\n",
      "Python development (1)\n",
      "Stats image (1)\n",
      "********************DF********************\n",
      "             Keywords_List\n",
      "0              ML used (3)\n",
      "1              DE used (3)\n",
      "2                 ML c (2)\n",
      "3                NLP c (2)\n",
      "4                 DE c (2)\n",
      "5                 DL c (2)\n",
      "6             Python c (2)\n",
      "7       ML programming (1)\n",
      "8       DL programming (1)\n",
      "9   Python programming (1)\n",
      "10             NLP may (1)\n",
      "11           Stats may (1)\n",
      "12              DL may (1)\n",
      "13          Python may (1)\n",
      "14         DE learning (3)\n",
      "15             ML deep (1)\n",
      "16         Python deep (1)\n",
      "17             ML data (3)\n",
      "18            NLP data (3)\n",
      "19          Stats data (3)\n",
      "20             DL data (3)\n",
      "21         Python data (3)\n",
      "22           NLP using (1)\n",
      "23  Python development (1)\n",
      "24         Stats image (1)\n",
      "['Shivam']\n",
      "******************DATAF**************\n",
      "   Candidate Name Subject       Keyword Count\n",
      "0          shivam      ML         used      3\n",
      "1          shivam      DE         used      3\n",
      "2          shivam      ML            c      2\n",
      "3          shivam     NLP            c      2\n",
      "4          shivam      DE            c      2\n",
      "5          shivam      DL            c      2\n",
      "6          shivam  Python            c      2\n",
      "7          shivam      ML  programming      1\n",
      "8          shivam      DL  programming      1\n",
      "9          shivam  Python  programming      1\n",
      "10         shivam     NLP          may      1\n",
      "11         shivam   Stats          may      1\n",
      "12         shivam      DL          may      1\n",
      "13         shivam  Python          may      1\n",
      "14         shivam      DE     learning      3\n",
      "15         shivam      ML         deep      1\n",
      "16         shivam  Python         deep      1\n",
      "17         shivam      ML         data      3\n",
      "18         shivam     NLP         data      3\n",
      "19         shivam   Stats         data      3\n",
      "20         shivam      DL         data      3\n",
      "21         shivam  Python         data      3\n",
      "22         shivam     NLP        using      1\n",
      "23         shivam  Python  development      1\n",
      "24         shivam   Stats        image      1\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "ML c (1)\n",
      "NLP c (1)\n",
      "DE c (1)\n",
      "DL c (1)\n",
      "Python c (1)\n",
      "ML data (2)\n",
      "NLP data (2)\n",
      "Stats data (2)\n",
      "DL data (2)\n",
      "Python data (2)\n",
      "********************DF********************\n",
      "     Keywords_List\n",
      "0         ML c (1)\n",
      "1        NLP c (1)\n",
      "2         DE c (1)\n",
      "3         DL c (1)\n",
      "4     Python c (1)\n",
      "5      ML data (2)\n",
      "6     NLP data (2)\n",
      "7   Stats data (2)\n",
      "8      DL data (2)\n",
      "9  Python data (2)\n",
      "['Umashankar', 'Resume']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject Keyword Count\n",
      "0     umashankar      ML      c      1\n",
      "1     umashankar     NLP      c      1\n",
      "2     umashankar      DE      c      1\n",
      "3     umashankar      DL      c      1\n",
      "4     umashankar  Python      c      1\n",
      "5     umashankar      ML   data      2\n",
      "6     umashankar     NLP   data      2\n",
      "7     umashankar   Stats   data      2\n",
      "8     umashankar      DL   data      2\n",
      "9     umashankar  Python   data      2\n",
      "*******************************************\n",
      "KEYWORDS\n",
      "NLP using (4)\n",
      "Stats image (1)\n",
      "ML processing (1)\n",
      "********************DF********************\n",
      "       Keywords_List\n",
      "0      NLP using (4)\n",
      "1    Stats image (1)\n",
      "2  ML processing (1)\n",
      "['VAISHALI BIJOY']\n",
      "******************DATAF**************\n",
      "   Candidate Name Subject      Keyword Count\n",
      "0  vaishali bijoy     NLP       using      4\n",
      "1  vaishali bijoy   Stats       image      1\n",
      "2  vaishali bijoy      ML  processing      1\n"
     ]
    }
   ],
   "source": [
    "#Code to execute the above functions \n",
    "final_db=pd.DataFrame()\n",
    "i=0\n",
    "while i < len(onlyfiles):\n",
    "    file=onlyfiles[i]\n",
    "    dat=create_profile(file)\n",
    "\n",
    "    final_db=final_db.append(dat)\n",
    "    i+=1\n",
    "    #print(final_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "KEYWORDS\n",
      "ML c (86)\n",
      "NLP c (86)\n",
      "DE c (86)\n",
      "DL c (86)\n",
      "Python c (86)\n",
      "********************DF********************\n",
      "   Keywords_List\n",
      "0      ML c (86)\n",
      "1     NLP c (86)\n",
      "2      DE c (86)\n",
      "3      DL c (86)\n",
      "4  Python c (86)\n",
      "['AbhayArora']\n",
      "******************DATAF**************\n",
      "  Candidate Name Subject Keyword Count\n",
      "0     abhayarora      ML      c     86\n",
      "1     abhayarora     NLP      c     86\n",
      "2     abhayarora      DE      c     86\n",
      "3     abhayarora      DL      c     86\n",
      "4     abhayarora  Python      c     86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neera\\anaconda3\\envs\\rasanewinstall\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate Name</th>\n",
       "      <th>DE</th>\n",
       "      <th>DL</th>\n",
       "      <th>ML</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Python</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abhayarora</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Candidate Name  DE  DL  ML  NLP  Python  mean\n",
       "0     abhayarora   1   1   1    1       1   1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=create_profile(onlyfiles[0])\n",
    "# data\n",
    "final = data['Keyword'].groupby([data['Candidate Name'], data['Subject']]).count().unstack()\n",
    "final.reset_index(inplace = True)\n",
    "final.fillna(0,inplace=True)\n",
    "candidate_data = final.iloc[:,1:]\n",
    "candidate_data.index = final['Candidate Name']\n",
    "#the candidate profile in a csv format\n",
    "cand=candidate_data.to_csv('candidate_profile.csv')\n",
    "cand_profile=pd.read_csv('candidate_profile.csv')\n",
    "cand_profile[\"mean\"]=cand_profile.mean(axis=1)\n",
    "\n",
    "DE=cand_profile[\"DE\"]\n",
    "DL=cand_profile[\"DL\"]\n",
    "NLP=cand_profile[\"NLP\"]\n",
    "ML=cand_profile[\"ML\"]\n",
    "Python=cand_profile[\"Python\"]\n",
    "mean=cand_profile[\"mean\"]\n",
    "name = cand_profile[\"Candidate Name\"]\n",
    "cand_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: DE, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    abhayarora\n",
       "Name: Candidate Name, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('rasanewinstall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5b456cce12072cd43e32011decf1dc387a911846745dda5970696409e68c92c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
